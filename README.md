# AdEMAMix Optimizer Implementation

# Introduction
This repository provides an implementation of the AdEMAMix optimizer, which is designed to improve upon the traditional Adam optimizer by utilizing a mixture of two Exponential Moving Averages (EMAs). AdEMAMix was proposed in the paper [The AdEMAMix Optimizer: Better, Faster, Older](https://arxiv.org/abs/2409.03137), and it leverages both recent and older gradients to enhance model convergence, particularly in complex architectures like Convolutional Neural Networks (CNNs) and Transformer models.
